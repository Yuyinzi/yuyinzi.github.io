---
title: 分布式锁的实现原理学习笔记
date: 2019-05-10 01:19:55
tags:
- 分布式
- ZooKeeper
- redis
- mysql
categories:
- 分布式
---

## 前言

在分布式场景下，单机的锁已经没有办法满足控制不同节点对同一资源的并发访问。

常见的分布式锁有三种：

- 基于`Mysql`
- 基于缓存(`redis`、`Memecached`)
- 基于`ZooKeeper`

<!--more-->

## 基于`Mysql`实现分布式锁

核心思想是：在数据库中创建一个表，表中包含**资源名**等字段，并在**资源名字段上创建唯一索引**，想要执行某个方法，就使用这个资源名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。

创建表：

```mysql
DROP TABLE IF EXISTS `resource_lock`;
CREATE TABLE `resource_lock` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键',
  `resource_name` varchar(64) NOT NULL COMMENT '资源名字',
  `desc` varchar(255) NOT NULL COMMENT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  UNIQUE KEY `unq_resource_name` (`resource_name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

要想获取某个资源，则使用该资源向表中插入插入数据。因为对资源惊醒了唯一性约束，因此数据库会保证只有一个操作成功，成功的操作可以看做拿到了锁。

想要释放某个锁时，利用`delete from`删除对应的行数据即可。

但是这个简单版本的方法存在一些不足：

- 无法实现阻塞的特性，获取不到锁时是直接返回，只能手动实现循环获取
- 无法实现可重入的特性，因此需要再加上`node_info`和`count`用来表示节点信息以及获取的锁个数。如果是同一节点，则获取时锁的个数加1；释放时，如果锁个数大于1，则减1，如果等于1可以直接删除该行
- 无法实现超时，如果获取锁的节点由于出现了宕机，将导致该锁永远无法释放。因此可以新增一列，设定超时时间，并在后台开启一个任务进行定期的回收
- 需要数据库保证可用，否则直接影响分布式锁的可用性及性能。所以可以通过双机部署，数据同步，主备切换等。

因此，虽然易于理解，但是实现起来需要考虑很多方面，与基于缓存实现的分布式锁相比性能较低。

## 基于`redis`实现分布式锁

通常使用`setnx key value`来实现，当返回1时表明这个`key`不存在，获取锁成功，否则为抢锁失败。

当任务执行完毕后，使用`del key`删除这个`key`，表明已经释放成功。

看起来很简便，但是会出现：

问题一：

同样没有锁超时，可能导致某个锁永远都不会被释放掉。因此需要在`setnx`使用`expires`紧跟其后设定超时。但是这样带来第二个问题是，这两个操作并不是原子性的，可能在还没执行`expires`时节点已经挂掉，那么锁仍然不会释放。

问题二：

`setnx`和`expire`操作不具有原子性。`redis 2.8`后可以使用`set key value ex 5 nx`来支持`nx`和`ex`是同一原子操作。

问题三：

如果获取到锁的`A`执行太久导致锁超时释放，那么当`B`获取到锁后执行过程中，`A`执行完了，再执行`del key`的操作，那么可能会将`B`的锁释放掉。也就是说，释放掉了不属于自己的锁。

可以通过在执行`set`时，将自己节点的信息写入，例如`set lock a_node ex 5 nx`。当释放锁时，先`get lock`查看值是不是自己的节点，如果是才能释放。可是这样又带来第四个问题。

问题四：

判断是否是自己的锁和释放锁的操作不是原子性的了。

1. 假设`A`获取锁成功，执行完毕后准备释放锁
2. 首先执行`get`获取`key`的值，是自己的锁，但是某些原因阻塞了
3. 锁超时了，`B`拿到了锁
4. `A`从阻塞中恢复，执行`del`，又把`B`的锁释放了

因此释放锁的操作必须使用`LUA`脚本实现。

问题五：

如果`A`获取了锁，但是执行时间很长，锁提前释放了，那么`A`接下来对资源操作的安全性将得不到保证。这个可以通过一个守护进程，发现要超时了就延长一下超时时间来解决。

问题六：

如果`redis`宕机了，所有客户端都无法获得锁。因此通常会使用`Master-slave`机制，但是主从复制是异步的。因此可能会出现：

1. `A`从`master`获取了锁
2. `master`挂了，`key`还没同步到`slave`上
3. `slave`升级为`master`
4. `B`从新的`master`获取到了锁

于是问题六是针对多`redis`节点的，只能使用`Redlock`来解决。

`Redlock`的大概原理：

1. 获取当前时间
2. 依次向`N`个节点执行获取锁操作，获取锁操作存在超时时间。如果获取某个节点锁失败，应该立即尝试下一个节点
3. 计算遍历完`N`个节点获取锁总共消耗的时间，只有从大于等于`N/2+1`个节点中成功获取到了锁，并且此时锁仍然没有到达超时时间，才认为锁获取成功
4. 如果最终获取锁成功，锁的有效时间等于最初有效时间减去取锁成功所所耗的时间
5. 如果获取锁失败了，应该向所有节点释放锁操作(同单节点相同，使用`LUA`脚本)

为了避免`redis`服务器短暂失效重新上线导致前一个节点的锁没有持久化却又被下一个节点所获得，还引入了延迟重启。即`redis`失效后，至少要等到`key`过期了，再重启。

为了防止实际上获得了锁，但是却没收到`redis`的`ack`而认为获得锁失败，在释放时应当对所有节点进行释放锁。

由于存在`N`个节点，只要能从大部分节点中获取锁，就可以视为获取锁成功，因此故障转移时发生的锁失效问题不存在了。但是程序执行时间过长导致锁过期的问题仍然没有解决。

## 基于`ZooKeeper`实现分布式锁

`ZooKeeper`实现分布式锁的步骤如下：

1. 创建一个目录`mylock`
2. 线程`A`想获取锁就在`mylock`目录下创建临时顺序节点； 
3. 获取`mylock`目录下所有的子节点，然后获取比自己编号小的节点，如果不存在，则说明当前线程顺序号最小，获得锁； 
4. 否则监听比自己编号小1的节点，等待其释放锁。

因此基于`zk`的分布式锁，不用考虑超时时间。因为一旦节点发生宕机(心跳检测)，服务器会自动删除该节点释放锁。同时，使用`ZooKeeper`也可以有效的解决不可重入的问题，客户端在创建节点的时候，**把当前客户端的主机信息和线程信息直接写入到节点中**，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。

`References`：

[基于 Redis 的分布式锁到底安全吗？](<https://juejin.im/post/58b3a93c1b69e60058b49767>)

[分布式锁简单入门以及三种实现方式介绍](<https://blog.csdn.net/xlgen157387/article/details/79036337>)

[再有人问你分布式锁，这篇文章扔给他](<https://juejin.im/post/5bbb0d8df265da0abd3533a5>)

[什么是分布式锁？](<https://juejin.im/post/5b16148a518825136137c8db>)

